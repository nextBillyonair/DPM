\section{Criterion}
The criterion and divergences listed here can be used to quantify the "distance"
between two distributions. Hence, in conjunction with torch optimizers,
one can minimize said difference to learn the paramters of a distribution. For
sake of notation clarity, $p$ is the true distribution and $q$ is the learned
distribution. Hence we "fit" $q$ to match $p$. In addition, we provide the
Monte Carlo approximation.

\subsection{Divergences}

\subsubsection{Cross-Entropy}
\begin{equation}
  \begin{aligned}
    H(p, q) =& - \int p(x) \log q(x) dx \\
    =& - \frac{1}{n} \sum_{x \sim p} \log q(x)
  \end{aligned}
\end{equation}
\subsubsection{Perplexity}
\begin{equation}
  \begin{aligned}
    H(p, q) =& \exp \left( - \int p(x) \log q(x) dx \right) \\
    =& \exp \left(- \frac{1}{n} \sum_{x \sim p} \log q(x) \right)
  \end{aligned}
\end{equation}
\subsubsection{Forward KL Divergence}
\begin{equation}
  \begin{aligned}
    H(p, q) =& \int p(x) \log \frac{p(x)}{q(x)} dx  \\
    =&  \frac{1}{n} \sum_{x \sim p} \log \frac{p(x)}{q(x)}
  \end{aligned}
\end{equation}
\subsubsection{Reverse KL Divergence}
\begin{equation}
  \begin{aligned}
    H(p, q) =& \int q(x) \log \frac{q(x)}{p(x)} dx  \\
    =&  \frac{1}{n} \sum_{x \sim q} \log \frac{q(x)}{p(x)}
  \end{aligned}
\end{equation}
\subsubsection{Jensen-Shannon Divergence}
\subsubsection{Earth Mover's Distance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ELBO}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Adversarial Loss}
Adversarial Losses are criterion functions that allow for sample-sample based
training between models $p$ and $q$. More formally, it hides a Discriminator
model that attempts to discriminate between the real data from $p$ and fake data
generated from $q$.
\subsubsection{Adversarial Loss}
\subsubsection{GAN Loss}
\subsubsection{MMGAN Loss}
\subsubsection{WGAN Loss}
\subsubsection{LSGAN Loss}
\subsubsection{Gradient Penalty}
\subsubsection{Spectral Norm}
