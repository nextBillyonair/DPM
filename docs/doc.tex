\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{array}
\usepackage{dsfont}
\usepackage{multirow, graphicx}
 \usepackage{float}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{caption}
\usepackage{subcaption}
\urlstyle{same}

\title{Differentiable Probabilistic Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  William Watson \\
  \texttt{nextbillyonair@gmail.com} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  d
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Minimum length: 4 pages. Maximum length: 8 pages !!!!!                   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\subsection{Philosophy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminary}
% all math not related to further sections
\subsection{Kronecker Product}
\subsection{Gradients}
\subsection{Jacobian}
\subsection{Hessian}
\subsection{Newton Optimization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributions}
\subsection{Distribution}
\subsection{Arcsine}
\subsection{Bernoulli}
\subsection{Beta}
\subsection{Categorical}
\subsection{Cauchy}
\subsection{Chi Square}
\subsection{Conditional Model}
\subsection{Convolution}
\subsection{Data}
\subsection{Dirac Delta}
\subsection{Dirichlet}
\subsection{Exponential}
\subsection{Fisher-Snedcor (F-Distribution)}
\subsection{Gamma}
\subsection{Generator}
\subsection{Gumbel Softmax}
\subsection{Gumbel}
\subsection{Half Cauchy}
\subsection{Half Normal}
\subsection{Hyperbolic Secant}
\subsection{Langevin}
\subsection{Laplace}
\subsection{Log Cauchy}
\subsection{Log Laplace}
\subsection{Log Normal}
\subsection{Logistic}
\subsection{Normal (Multivariate)}
\subsection{Rayleigh}
\subsection{Relaxed Bernoulli}
\subsection{Student T}
\subsection{Transformed Distribution}
\subsection{Uniform}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mixture Models}
Mixture Models are probabilistic models that use other distributions as sub-components.
These sub-components are weighted in order to form a mixture of the components.
\subsection{Mixture Model}
\subsection{Gumbel Mixture Model}
\subsection{Infinite Mixture Model}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Transforms}
Transforms are invertible functions that can be applied to a
random variable to change the distribution.
\subsection{Transform}
\subsection{Inverse Transform}
\subsection{Affine}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item Location $\mu \in \mathbb{R}^n$
    \item Scale $\sigma > 0$
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = \mu + \sigma \cdot x
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \frac{y - \mu}{\sigma}
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = \log \vert \sigma \vert
  \end{equation}
\end{itemize}
\subsection{Exp}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item None
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = e^x
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \log y
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = x
  \end{equation}
\end{itemize}
\subsection{Expm1}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item None
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = e^x - 1
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \log ( 1 + y )
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = x
  \end{equation}
\end{itemize}
\subsection{Gumbel}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item Location $\mu \in \mathbb{R}^n$
    \item Scale $\sigma > 0$
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = \exp \left( - \exp \left( - \frac{x - \mu}{\sigma} \right) \right)
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \mu - \sigma \cdot \log \left( - \log \left( y \right) \right)
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = - \log \left( \frac{\sigma}{-\log(y) \cdot y} \right)
  \end{equation}
\end{itemize}
\subsection{Log}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item None
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = \log x
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \exp y
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = - y
  \end{equation}
\end{itemize}
\subsection{Logit}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item None
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = \log \left( \frac{x}{1 - x} \right)
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \frac{1}{1 + e^{-y}}
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = \log\left( 1 + e^{-y} \right) + \log\left( 1 + e^{y} \right)
  \end{equation}
\end{itemize}
\subsection{Power}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item Power $p$
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = \begin{cases} e^x & p = 0 \\ \left( 1 + x \cdot p \right) ^ {1 / p} & \text{otherwise} \end{cases}
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \begin{cases} \log y & p = 0 \\ y^{p - 1} / p & \text{otherwise} \end{cases}
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = \begin{cases} x & p = 0 \\ \left(\frac{1}{p} - 1 \right) \cdot \log \left(x \cdot p + 1 \right) & \text{otherwise} \end{cases}
  \end{equation}
\end{itemize}
\subsection{Reciprocal}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item None
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = 1 / x
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = 1 / y
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = -2 \cdot \log \vert x \vert
  \end{equation}
\end{itemize}
\subsection{Sigmoid}
\begin{itemize}
  \item \textbf{Parameters}
  \begin{itemize}
    \item None
  \end{itemize}
  \item \textbf{Forward}
  \begin{equation}
    f(x) = \frac{1}{1 + e^{-x}}
  \end{equation}
  \item \textbf{Inverse}
  \begin{equation}
    f^{-1}(y) = \log \left( \frac{y}{1 - y} \right)
  \end{equation}
  \item \textbf{Log Absolute Determinant Jacobian}
  \begin{equation}
    \log \vert \text{det} \, \mathbf{J} \vert (x, y) = -\log\left( 1 + e^{-x} \right) - \log\left( 1 + e^{x} \right)
  \end{equation}
\end{itemize}
\subsection{SinhArcsinh}
\subsection{Softplus}
\subsection{Softsign}
\subsection{Square}
\subsection{Tanh}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Criterion and Divergences}
The criterion and divergences listed here can be used to quantify the "distance"
between two distributions. Hence, in conjunction with torch optimizers,
one can minimize said difference to learn the paramters of a distribution. For
sake of notation clarity, $p$ is the true distribution and $q$ is the learned
distribution. Hence we "fit" $q$ to match $p$. In addition, we provide the
Monte Carlo approximation.
\subsection{Cross-Entropy}
\begin{equation}
  \begin{aligned}
    H(p, q) =& - \int p(x) \log q(x) dx \\
    =& - \frac{1}{n} \sum_{x \sim p} \log q(x)
  \end{aligned}
\end{equation}
\subsection{Perplexity}
\begin{equation}
  \begin{aligned}
    H(p, q) =& \exp \left( - \int p(x) \log q(x) dx \right) \\
    =& \exp \left(- \frac{1}{n} \sum_{x \sim p} \log q(x) \right)
  \end{aligned}
\end{equation}
\subsection{Forward KL Divergence}
\begin{equation}
  \begin{aligned}
    H(p, q) =& \int p(x) \log \frac{p(x)}{q(x)} dx  \\
    =&  \frac{1}{n} \sum_{x \sim p} \log \frac{p(x)}{q(x)}
  \end{aligned}
\end{equation}
\subsection{Reverse KL Divergence}
\begin{equation}
  \begin{aligned}
    H(p, q) =& \int q(x) \log \frac{q(x)}{p(x)} dx  \\
    =&  \frac{1}{n} \sum_{x \sim q} \log \frac{q(x)}{p(x)}
  \end{aligned}
\end{equation}
\subsection{Jensen-Shannon Divergence}
\subsection{Earth Mover's Distance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ELBO}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adversarial Loss}
Adversarial Losses are criterion functions that allow for sample-sample based
training between models $p$ and $q$. More formally, it hides a Discriminator
model that attempts to discriminate between the real data from $p$ and fake data
generated from $q$.
\subsection{Adversarial Loss}
\subsection{GAN Loss}
\subsection{MMGAN Loss}
\subsection{WGAN Loss}
\subsection{LSGAN Loss}
\subsection{Gradient Penalty}
\subsection{Spectral Norm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Models}
\subsection{Base Models}
\subsubsection{Model}

\subsection{Regression}
\subsubsection{Linear Regression (Normal)}
\subsubsection{L1 Regression (Laplace)}
\subsubsection{Ridge Regression (Normal + Normal Prior on Weights)}
\subsubsection{Lasso Regression (Normal + Laplace Prior on Weights)}

\subsection{Classification}
\subsubsection{Logistic Regression (Bernoulli)}
\subsubsection{Bayesian Logistic Regression (Bernoulli)}
\subsubsection{Softmax Regression (Categorical)}

\subsection{Generative Classifiers}
\subsection{Bernoulli Naive Bayes (Bernoulli - Bernoulli)}
\subsection{Gaussian Naive Bayes (Multinomial - Gaussian)}
\subsection{Multinomial Naive Bayes (Bernoulli - Multinomial)}
\subsection{Linear Discriminant Analysis (Multinomial - Shared Covariance)}
\subsection{Quadratic Discriminant Analysis (Multinomial - Multivariate Gaussian)}

\subsection{Clustering}
\subsubsection{Gaussian Mixture Model}

\subsection{Unconstrained Matrix Factorization (Gaussian)}

\subsection{Principle Components Analysis}

\subsection{Generative Adversarial Networks}
\subsubsection{Generative Adversarial Networks}
\subsubsection{GAN Model}
\subsubsection{MMGAN Model}
\subsubsection{WGAN Model}
\subsubsection{LSGAN Model}

\subsection{Variational Auto-Encoders (TBD)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Monte Carlo}
\subsection{Monte Carlo Approximation}
\subsection{Linear Congruential Generator}
\subsection{Inverse Transform Sampling}
\subsection{Box-Muller}
\subsection{Marsaglia-Bray}
\subsection{Rejection Sampling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Chain Monte Carlo (MCMC)}
\subsection{Metropolis}
\subsection{Metropolis-Hastings}
\subsection{Metropolis-Adjusted Langevin Algorithm (MALA)}
\subsection{Hamiltonian Monte Carlo}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ------------------------------



% \bibliographystyle{abbrv}
% \bibliography{report}


\end{document}
