\documentclass{beamer}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{array}
\usepackage{dsfont}
\usepackage{multirow, graphicx}
 \usepackage{float}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{caption}
\usepackage{subfig}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{algorithm,algorithmic}
% \floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\DeclareMathOperator*{\argmin}{argmin}
\urlstyle{same}
\usepackage{listings}
% \usetheme{Boadilla}

\title{Introduction to Random Numbers, Sampling,\\and MCMC Methods}
% \subtitle{Using Beamer}
\author{Bill Watson}
\institute{S\&P Global}
\date{\today}

\newenvironment{nospaceflalign*}
 {\setlength{\abovedisplayskip}{0pt}\setlength{\belowdisplayskip}{0pt}%
  \csname flalign*\endcsname}
 {\csname endflalign*\endcsname\ignorespacesafterend}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\begin{document}

\begin{frame}
\titlepage
\end{frame}


\begin{frame}
\frametitle{What is Sampling and why is it useful?}
\begin{itemize}
  \item Sampling is the practice of generating observations from a population
  \item Monte Carlo methods are algorithms that rely on repeated random sampling
    to obtain approximations where it is difficult or impossible to use deterministic approaches
    \begin{itemize}
      \item Optimization
      \item Numerical Integration
      \item Sampling distributions
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Application: Approximating $\pi$}
\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE Batch Size $N$
  \STATE Sample $u_1 \sim \text{Uniform}(0, 1)$ $N$ times
  \STATE Sample $u_2 \sim \text{Uniform}(0, 1)$ $N$ times
  \STATE $\tilde{\pi} = \frac{4}{N} \cdot \left\vert \left\{ (u_1, u_2) \; \Big \vert \; \sqrt{u_1^2 + u_2^2} < 1 \right\} \right\vert$
  \ENSURE $\tilde{\pi}$
\end{algorithmic}
\caption{Approximating $\pi$}
\end{algorithm}
\end{frame}


\begin{frame}
  \frametitle{Application: Approximating $\pi$}
  \begin{figure}
    \centering
    \includegraphics[scale=0.4]{assets/approx_pi_circle}
    \caption{The ratio of points inside the unit circle approximates $\frac{\pi}{4}$}
  \end{figure}
\end{frame}


\begin{frame}
  \frametitle{Application: Approximating $\pi$}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/approx_pi_error}
  \end{figure}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{What can we do with our samples?}
\begin{itemize}
  \item Integration: $\int p(x) \, f(x) \, dx \approx \frac{1}{n} \sum_{x_i \sim P} f(x_i) $
  \item Expectation: $ \mu \approx \frac{1}{n} \sum_{x_i} x_i $
  \item Variance: $ \sigma^2 \approx \frac{1}{n} \sum_{x_i} \left( x_i - \mu \right)^2$
  \item Median: $\text{median} \approx \text{median} (x_1, x_2, \hdots x_n)$
  \item Entropy: $\mathbb{H}(P) \approx - \frac{1}{n} \sum_{x_i \sim P} \log p (x_i)$
  \item CDF: $p(c) \approx \frac{1}{n} \, \vert \{ x_i \, | \, x_i \leq c \} \vert$
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pseudo-Random Number Generators}
\begin{frame}
\frametitle{Pseudo-Random Number Generators}
\begin{itemize}
  \item If we need random numbers, then how do we generate them?
  \item One solution: Pseudo-Random Number Generators
  \item Pseudo since they cannot simulate "true" randomness
  \item But can be replicated via "seeds"
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Pseudo-Random Number Generators: LCG}
\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE Modulus $m$, Multiplier $a$, Increment $c$, Seed $X_0$
  \STATE $X_{i+1} = \left( a \cdot X_i + c \right) \mod m$
  \ENSURE $X_{i+1}$
\end{algorithmic}
\caption{Linear Congruential Generator}
\end{algorithm}
\end{frame}


\begin{frame}
  \frametitle{Pseudo-Random Number Generators}
  \begin{itemize}
    \item LCGs are of low quality
    \item Mersenne Twister (1998) is the first PRNG to avoid major problems and run quickly
    \item Can also use Hardware (True) RNG, External Entropy PRNGs
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inverse Transform Sampling}
\begin{frame}
\frametitle{Generating a Normal from the CDF}
\begin{itemize}
  \item We can use the cumulative distribution function to sample any distribution
  \item For instance, a normal's CDF is:
\end{itemize}
\begin{equation*}
  F(x) = \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{x - \mu}{\sigma \sqrt{2}} \right) \right]
\end{equation*}
\begin{itemize}
  \item With an Inverse CDF as:
\end{itemize}
\begin{equation*}
  F^{-1}(p) = \mu + \sigma \sqrt{2} \cdot \text{erf}^{-1}\left( 2p - 1 \right)
\end{equation*}
\begin{itemize}
  \item $\text{erf}(x)$ is the error function, defined as:
\end{itemize}
\begin{equation*}
  \text{erf}(x) = \frac{2}{\sqrt{\pi}} \int^{x}_{0} e^{-t^2} dt
\end{equation*}
\end{frame}


\begin{frame}
\frametitle{Inverse Transform Sampling}
\begin{itemize}
  \item It's easy to generalize this method to any distribution with a closed-form inverse CDF
\end{itemize}
\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE Inverse CDF $F^{-1}$
  \STATE Sample $u \sim \text{Uniform}(0, 1)$
  \STATE $X = F^{-1} (u)$
  \ENSURE $X$
\end{algorithmic}
\caption{Inverse Transform Sampling}
\end{algorithm}
\end{frame}

% delete?
\begin{frame}
  \frametitle{Inverse Transform Sampling: Intuition}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/icdf_transform}
  \end{figure}
\end{frame}


\begin{frame}
\frametitle{Table of Inverse CDFs for Common Distributions}
\begin{center}
\begin{tabular}{L{2cm}C{3.2cm}C{4.2cm}}
\toprule
{Distribution} & $F(x)$ & $F^{-1}(p)$ \\
\midrule
$\mathcal{N}(\mu, \sigma)$   & $\frac{1}{2} \left[ 1 + \text{erf} \left( \frac{x - \mu}{\sigma \sqrt{2}} \right) \right]$ & $\mu + \sigma \sqrt{2} \cdot \text{erf}^{-1}\left( 2p - 1 \right)$ \\
\\
$\mathcal{U}(a, b)$       & $\frac{x - a}{b - a}$ & $a + p \cdot \left( b - a \right)$ \\
\\
$\text{Exp}(\lambda)$        & $1 - e^{-\lambda x}$  & $\frac{- \ln \left( 1 - p \right)}{\lambda}$ \\
\\
$\text{Logistic}(\mu, s)$    & $\frac{1}{1+e^{-\frac{x-\mu}{s}}}$ & $\mu + s \ln \left( \frac{p}{1-p} \right)$\\
\bottomrule
\end{tabular}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Rejection Sampling}
\begin{frame}
  \frametitle{Rejection Sampling}
  \begin{itemize}
    \item What if we do not have a closed from CDF or ICDF?
    \item We can instead use Rejection Sampling!
  \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Rejection Sampling: Algorithm}
\begin{algorithm}[H]
\begin{algorithmic}[1]
  \REQUIRE Model $F$, Proposal $G$, $M > 1$
  \STATE Sample $x \sim G$
  \STATE Sample $u \sim \text{Uniform}(0, 1)$
  \IF {$u < \frac{f(x)}{M \cdot g(x)}$}
    \STATE {Accept $x$}
  \ELSE
    \STATE {Reject $x$}
  \ENDIF
  \ENSURE Accepted Samples
\end{algorithmic}
\caption{Rejection Sampling}
\end{algorithm}
\end{frame}

\begin{frame}
  \frametitle{Rejection Sampling: Intuition}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/rejection_sampling_explanation}
  \end{figure}
\end{frame}


\begin{frame}
  \frametitle{Rejection Sampling: Example}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/rs_m_1_3}
    \caption{Rejection Sampling with $M = 1.3$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Rejection Sampling: Example}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/rs_hist_1_3}
    \caption{$M = 1.3$, $6,660$ Accepted Samples}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Rejection Sampling: Example}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/rs_m_2_5}
    \caption{Rejection Sampling with $M = 2.5$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Rejection Sampling: Example}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/rs_hist_2_5}
    \caption{$M = 2.5$, $4,034$ Accepted Samples}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Rejection Sampling: Example}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/rs_m_100}
    \caption{Rejection Sampling with $M = 100$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Rejection Sampling: Example}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/rs_hist_100}
    \caption{$M = 100$, $79$ Accepted Samples}
  \end{figure}
\end{frame}


\begin{frame}
  \frametitle{Rejection Sampling: Pros \& Cons}
  \begin{itemize}
    \item Pros:
    \begin{itemize}
      \item Can be more efficient if the CDF is intractable
    \end{itemize}
    \item Cons:
    \begin{itemize}
      \item Tuning $M$ can be difficult. Too high and we reject too many, too low and
      we under approximate our target
      \item Very inefficient in higher dimensions
    \end{itemize}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Markov Chain Monte Carlo (MCMC)}
\begin{frame}
\frametitle{What is MCMC?}
\begin{itemize}
  \item Idea: Construct a Markov chain whose stationary distribution is the
  target density of interest, $f(x)$.
  \item The more steps we take in the chain, the better the approximation.
  \item This method works well with multi-dimensional continuous variables.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Metropolis-Hastings}
  \begin{columns}
  \begin{column}{0.5\textwidth}
  \begin{algorithm}[H]
  \algsetup{linenosize=\tiny}
  \scriptsize
  \begin{algorithmic}[1]
    \REQUIRE Model $F$, Proposal $G$
    \STATE Initialize $x_0$
    \FOR {$s = 0, 1, \hdots$}
      \STATE Sample $x' \sim g(x'|x_s)$
      \STATE Compute acceptance probability
        \begin{nospaceflalign*}
          r = \min\left( 1, \; \frac{f(x')}{f(x_s)} \frac{g(x_s|x')}{g(x'|x_s)} \right) &&
        \end{nospaceflalign*}
      \STATE Sample $u \sim \text{Uniform}(0, 1)$
      \IF {$u < r$}
        \STATE {Accept $x'$}
        \STATE {Set $x_{s+1} = x'$}
      \ELSE
        \STATE{Reject $x'$}
        \STATE {Set $x_{s+1} = x_s$}
      \ENDIF
    \ENDFOR
    \ENSURE Accepted Samples
  \end{algorithmic}
  \caption{Metropolis-Hastings Algorithm}
  \end{algorithm}
  \end{column}
  \begin{column}{0.5\textwidth}
    \begin{itemize}
      \item Construct a Markov Chain where we propose a new state $x'$ from the
      current state $x_s$ with probability $g(x'|x_s)$.
      \item After drawing a proposal $x'$ we calculate an acceptance probability, and
      if accepted, update the state to $x'$, else stay at state $x_s$.
    \end{itemize}
  \end{column}
  \end{columns}
\end{frame}


\begin{frame}
  \frametitle{Metropolis-Hastings: Example}
  \begin{figure}
    \centering
    \includegraphics[scale=0.35]{assets/mcmc_0_1}
    \caption{$\sigma^2 = 0.1$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Metropolis-Hastings: Example}
  \begin{figure}
    \centering
    \includegraphics[scale=0.35]{assets/mcmc_3}
    \caption{$\sigma^2 = 3$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Metropolis-Hastings: Example}
  \begin{figure}
    \centering
    \includegraphics[scale=0.35]{assets/mcmc_10}
    \caption{$\sigma^2 = 10$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Metropolis-Hastings: Example}
  \begin{figure}
    \centering
    \includegraphics[scale=0.35]{assets/mcmc_100}
    \caption{$\sigma^2 = 100$}
  \end{figure}
\end{frame}


\begin{frame}
\frametitle{Metropolis-Hastings: Key Terms to Know}
\begin{itemize}
  \item Acceptance Rates
  \begin{itemize}
    \item Fraction of draws that are accepted
    \item High acceptance rate $\rightarrow$ bad mixing
    \item Low Acceptance rate $\rightarrow$ inefficient
    \item Theoretical rates: $44\%$ for one dimension, $23.4\%$ as the dimension goes to infinity
  \end{itemize}
  \item Chains
  \item Burn In
  \begin{itemize}
    \item Allows the chain to "forget" its starting values and converge on areas of high probabilty
  \end{itemize}
  \item Mixing
  \begin{itemize}
    \item Allowing the chains to fully explore the state space, instead of collapsing
    in one peak
  \end{itemize}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Variants of Metropolis-Hastings}
\begin{frame}
  \frametitle{(Random Walk) Metropolis Algorithm}
  \begin{itemize}
    \item Uses a symmetric proposal distribution $G$, such that \\
    $g(x_s|x') = g(x'|x_s)$
    \item Our acceptance probability $r$ is then
  \end{itemize}
  \begin{gather*}
    r = \min \left(1 , \frac{f(x')}{f(x_s)} \right)
  \end{gather*}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Adjusted Langevin Algorithm (MALA)}
  \begin{itemize}
    \item New states are proposed with Langevin dynamics
  \end{itemize}
  \begin{gather*}
      x' = x_s + \tau \nabla \log f(x_s) + \sqrt{2 \tau} \xi_k
  \end{gather*}
  \begin{itemize}
    \item Proposal probabilities are normally distributed as
  \end{itemize}
  \begin{gather*}
    g(x'| x_s) \sim \mathcal{N}(x_s + \tau \nabla \log f(x_s), \, 2 \tau  I_d) \\
    g(x_s | x') \sim \mathcal{N}(x' + \tau \nabla \log f(x'), \, 2 \tau  I_d)
  \end{gather*}
  \begin{itemize}
    \item Optimal acceptance rate is $57.4\%$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Adjusted Langevin Algorithm: Example}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/mcmc_lang_visual}
    \caption{MALA: $\tau=0.4$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte Carlo}
  \begin{itemize}
    \item Inspired by using a Hamiltonian dynamics evolution simulated using
    a time-reversible and volume preserving leapfrog integrator.
    \item Purpose was to reduce the correlation between successive sample states by proposing
    moves to distant states with high probability of acceptance.
    \item In simplier terms: flick a puck, wait, stop, then hit it again
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Dynamics}
  \begin{itemize}
    \item For a system with state $q$, momentum $p$:
  \begin{gather*}
    H(q, p) = U(q) + K(p) \\
    U(q) = - \log f(q) \quad\quad\quad K(p) = \sum_i \frac{p_i^2}{2}
  \end{gather*}
    \item The time evolution of the system is defined by:
  \begin{gather*}
    \frac{d p}{d t} = - \frac{\partial H}{\partial q} = - \frac{\partial U(q)}{\partial q} \quad\quad\quad \frac{d q}{d t} = \frac{\partial H}{\partial p} = p
  \end{gather*}
    \item We can update the system coordinates as follows (leapfrog):
  \begin{gather*}
    q_{i+1} = q_i + \epsilon p_i \quad\quad\quad p_{i+1} = p_i - \epsilon \frac{\partial U(q_{i+1})}{\partial q_{i+1}}
  \end{gather*}
    \item Our acceptance probability for the current state $(q_s, p_s)$ and candidate $(q, p)$ is
  \begin{equation*}
    r = \exp \left( H(q_s, p_s) - H(q, p) \right)
  \end{equation*}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Hamiltonian Monte Carlo: Algorithm}
  \begin{algorithm}[H]
  \algsetup{linenosize=\tiny}
  \scriptsize
  \begin{algorithmic}[1]
    \REQUIRE Model $F$, Stepsize $\epsilon$, Leapfrog Steps $L$, Current State $x_s$
      \STATE Set $q = q_s$, $p \sim \mathcal{N}(0, 1)$, $p_s = p$
      \STATE $p = p - \epsilon \frac{\partial U (q)}{\partial q} / 2$
      \FOR {$l = 0, 1, \hdots L$}
        \STATE $q = q + \epsilon \cdot p$
        \STATE $p = p - \epsilon \frac{\partial U (q)}{\partial q}$ except at end of trajectory
      \ENDFOR
      \STATE $p = p -\epsilon \frac{\partial U (q)}{\partial q} / 2$
      \STATE $p = -p$ to make proposal symmetric
      \STATE Compute acceptance probability
        \begin{nospaceflalign*}
          r = \exp \left( U(q_s) - U(q) + K(p_s) - K(p)  \right) &&
        \end{nospaceflalign*}
      \STATE Sample $u \sim \text{Uniform}(0, 1)$
      \IF {$u < r$}
        \RETURN {$q$}
      \ELSE
        \RETURN {$q_s$}
      \ENDIF
  \end{algorithmic}
  \caption{HMC, Single Candidate Update}
  \end{algorithm}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte Carlo: The Leapfrog Path}
  % put leapfrog visual
  \begin{figure}
    \centering
    \includegraphics[scale=0.3]{assets/hmc_leapfrog}
    \caption{Leapfrog Paths: $\epsilon=0.3$, $L=60$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte Carlo: Tampering to Overcome Energy Barriers}
  % put leapfrog visual
  \begin{figure}
    \centering
    \includegraphics[scale=0.3]{assets/hmc_tampering}
    \caption{Leapfrog Paths: $\epsilon=0.2$, $L=50$, $\alpha=1.05$}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte Carlo: Example}
  % put mcmc result
  % go to linked visual
  \begin{figure}
    \centering
    \includegraphics[scale=0.3]{assets/hmc_hist}
    \caption{HMC: $\epsilon=0.6$, $L=40$, $\alpha=1.05$, 100 burn in iterations, 10,000 epochs}
  \end{figure}
\end{frame}


\begin{frame}
  \frametitle{Hamiltonian Monte Carlo: Considerations}
  \begin{itemize}
    \item Distances between points are large, thus requiring less iterations
    \item Mostly accepts new states, more efficient even with the leapfrog "price"
    \item Tuning leapfrog steps can be difficult:
    \begin{itemize}
      \item Small $L \rightarrow$ random walk behavior
      \item Large $L \rightarrow$ wasted computation
    \end{itemize}
    \item Has trouble sampling from distributions with isolated local minimums (lack of energy to cross the energy barrier)
    \item Optimal acceptance rate is $65\%$
    \item \href{https://chi-feng.github.io/mcmc-demo/app.html}{HMC Interactive Demo}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{No-U-Turn Sampler (NUTS)}
  \begin{itemize}
    \item Removes the need to step the leapfrog step $L$ in HMC
    \item Uses a recursive algorithm to build a set of likely candidate points
    \item As efficient as a well tuned HMC method
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{General Error Bounds}
\begin{frame}
  \frametitle{Analysis of Error}
  \begin{itemize}
    \item How many samples $S$ does it take to approximate the target distribution "well"?
    \item Answer: Use the Hoeffding bound
  \end{itemize}
  \begin{equation*}
    Pr \left( \hat{p}(x) \not\in \left[p(x) - \epsilon, p(x) + \epsilon \right] \right) \leq 2 e^{-2S \epsilon^2}
  \end{equation*}
  \begin{itemize}
    \item For the number of samples $S$, an error bound $\epsilon$ with probability $1 - \delta$,
      we can solve:
  \end{itemize}
  \begin{gather*}
    2e^{-2S \epsilon^2} \leq \delta \\
    S \geq \frac{\log(2 / \delta)}{2 \epsilon^2}  \\
  \end{gather*}
\end{frame}

\begin{frame}
  \frametitle{Analysis of Error}
  \begin{itemize}
    \item We can also use the Chernoff Bound relative to the true value $p(x)$
    \item However, this is dependent on $p(x)$, which is not always known.
  \end{itemize}
  \begin{gather*}
    Pr \left( \hat{p}(x) \not\in \left[p(x)(1 - \epsilon), p(x)(1 + \epsilon) \right] \right) \leq 2 e^{-S p(x) \epsilon^2 / 3} \\
    S \geq 3 \frac{\log \left(2 / \delta \right)}{p(x) \epsilon^2} \\
  \end{gather*}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tools, References, and Further Reading}

\begin{frame}
  \frametitle{Libraries and Tools}
  \begin{itemize}
    \item \href{https://pymc-devs.github.io/pymc3/}{PyMC3}
    \item \href{https://www.tensorflow.org/probability/}{TensorFlow Probability}
    \item \href{http://mc-stan.org}{Stan}
    \item \href{https://mc-stan.org/users/interfaces/rstan}{RStan}
    \item \href{https://cran.r-project.org/web/packages/mcmc/index.html}{R mcmc}
  \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Refrences \& Further Reading}
  \begin{itemize}
    \item \href{https://www.cs.ubc.ca/~murphyk/MLbook/}{Machine Learning: A Probabilistic Perspective by Kevin Murphy}
    \item \href{https://www.springer.com/gp/book/9780387004518}{Monte Carlo Methods in Financial Engineering by Paul Glasserman}
    \item \href{https://mitpress.mit.edu/books/probabilistic-graphical-models}{Probabilistic Graphical Models: Principles and Techniques by Daphne Koller and Nir Friedman}
    \item Sampling Lecture from my PGM Professor Daniel Malinsky
    \item \href{http://www.mcmchandbook.net/HandbookChapter5.pdf}{MCMC Using Hamiltonian Dynamics by Radford M. Neal}
    \item \href{http://www.cs.utoronto.ca/~radford/ftp/review.pdf}{Probabilistic Inference Using Markov Chain Monte Carlo Methods by Radford M. Neal}
    \item \href{http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html}{Hamiltonian Monte Carlo Explained by Alex Rogozhnikov}
    \item \href{https://chi-feng.github.io/mcmc-demo/app.html}{The Markov-chain Monte Carlo Interactive Gallery by Chi Feng}
    \item \href{http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf}{The No-U-Turn Sampler by Hoffman and Gelman}
  \end{itemize}
\end{frame}

% Refs, ideas, etc


\end{document}
